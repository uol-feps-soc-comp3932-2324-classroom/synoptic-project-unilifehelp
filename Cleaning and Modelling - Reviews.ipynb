{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_33520\\3095782909.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Importing metadata zip file and converting it to dataframe \n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "def getDF(path):\n",
    "  # Unzip the file, load in each line as an object\n",
    "  g = gzip.open(path, 'rb')\n",
    "  g = [json.loads(l) for l in g]\n",
    "\n",
    "  # Map to a dictionary, then load in as a dataframe\n",
    "  dict_df = {i: d for (i, d) in enumerate(g)}\n",
    "  return pd.DataFrame.from_dict(dict_df, orient='index')\n",
    "\n",
    "df = getDF('All_Beauty.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>asin</th>\n",
       "      <th>newReviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>319639</td>\n",
       "      <td>319639</td>\n",
       "      <td>319639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>319639</td>\n",
       "      <td>31438</td>\n",
       "      <td>319639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>My  husband wanted to reading about the Negro ...</td>\n",
       "      <td>B000FOI48G</td>\n",
       "      <td>My  husband wanted to reading about the Negro ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>8268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reviewText        asin  \\\n",
       "count                                              319639      319639   \n",
       "unique                                             319639       31438   \n",
       "top     My  husband wanted to reading about the Negro ...  B000FOI48G   \n",
       "freq                                                    1        8268   \n",
       "\n",
       "                                            newReviewText  \n",
       "count                                              319639  \n",
       "unique                                             319639  \n",
       "top     My  husband wanted to reading about the Negro ...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic cleaning steps\n",
    "\n",
    "# Remove empty or null descriptions\n",
    "df = df.drop(df[ ( (df[\"reviewText\"] == \"\" ) | ( df[\"reviewText\"].isnull()) ) ].index)\n",
    "\n",
    "# Selecting only the columns that are required for analysis\n",
    "colums_reviewtext_asin = [\"reviewText\", \"asin\"]\n",
    "df = df[colums_reviewtext_asin]\n",
    "\n",
    "# Removing \\n\n",
    "df[\"newReviewText\"] = df.reviewText.map(lambda x: x.replace(\"\\n\",\"\"))\n",
    "\n",
    "\n",
    "# Remove all duplicate of reviews so can have only unique reviews for classifier\n",
    "df[df.duplicated(\"newReviewText\")]\n",
    "df = df.drop_duplicates(\"newReviewText\", keep=\"last\")\n",
    "# Matches number of unique reviews now\n",
    "df.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                210657\n",
       "unique                                               210657\n",
       "top       My  husband wanted to reading about the Negro ...\n",
       "freq                                                      1\n",
       "Name: newReviewText, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove reviews within set limit of words\n",
    "\n",
    "# Split reviews at white spaces\n",
    "df[\"num_words_reviews\"] = df[\"newReviewText\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Remove reviews that don't meet this criteria\n",
    "df = df[(df[\"num_words_reviews\"] <= 181) & (df[\"num_words_reviews\"] >= 14)]\n",
    "\n",
    "df[\"newReviewText\"].describe() #210657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'throughout', 'me', 'beside', 'namely', 'go', 'less', 'are', 'everything', 'so', 'sometimes', 'seemed', 'but', 'keep', 'become', 'should', 'cannot', 'is', 'than', 'seems', 'get', 'more', 'can', 'who', 'quite', 'around', 'even', 'might', 'not', 'top', 'neither', 'itself', 'whom', 'any', 'at', 'yourself', 'other', 'hereupon', 'nevertheless', 'wherever', 'via', 'only', 'somewhere', 'perhaps', 'made', 'yet', 'see', '‘ll', '‘m', 'thereafter', 'an', 're', 'where', '’ll', 'here', 'most', 'such', 'bottom', 'there', 'well', '‘ve', 'just', 'about', 'twelve', 'does', 'whereafter', 'eleven', 'else', 'using', 'ourselves', 'on', 'latterly', 'besides', 'that', 'whoever', 'into', 'or', 'sometime', 'becomes', 'had', 'us', 'almost', 'ca', 'someone', 'least', \"'ll\", 'somehow', 'due', 'n’t', 'whereas', 'move', 'hundred', 'again', 'sixty', 'also', 'against', 'thereby', 'serious', 'our', 'afterwards', '’ve', 'anyone', 'rather', 'everyone', 'themselves', 'nowhere', 'anyway', 'above', \"'s\", 'something', 'take', 'he', 'him', 'them', 'to', 'mostly', 'another', 'next', 'latter', 'six', 'their', 'thus', 'am', 'up', 'otherwise', 'may', '‘re', 'mine', 'ever', 'himself', 'upon', 'yourselves', 'therein', 'below', \"n't\", 'forty', 'you', 'as', 'though', 'used', '‘d', 'was', '’m', 'hers', 'fifteen', 'each', 'formerly', 'already', 'your', 'with', 'few', 'the', 'they', 'too', 'seeming', 'and', 'hence', 'some', \"'re\", 'per', 'whereby', 'a', 'be', 'last', 'will', 'down', 'make', 'together', 'between', '‘s', 'one', 'becoming', 'every', 'thereupon', 'front', \"'ve\", 'indeed', 'those', 'herself', 'thru', 'has', 'why', 'whatever', 'whence', 'among', 'through', 'whether', 'hereafter', 'much', 'hereby', 'we', 'noone', 'these', 'toward', 'call', \"'m\", 'put', 'under', 'within', 'ten', 'seem', 'nothing', 'therefore', 'show', 'meanwhile', 'always', 'out', 'nobody', 'thence', 'both', 'herein', 'two', 'after', 'yours', 'then', 'which', 'twenty', 'back', 'first', 'because', 'except', 'own', 'either', 'many', 'really', 'became', 'until', 'ours', 'over', 'while', 'former', 'side', 'behind', 'whole', 'this', 'anywhere', 'n‘t', 'say', 'three', 'its', 'third', 'wherein', 'in', 'very', 'myself', 'further', 'never', 'if', 'it', 'i', 'whenever', 'elsewhere', 'although', 'enough', 'before', 'various', 'do', 'none', 'eight', 'alone', 'no', 'empty', 'by', 'were', 'full', 'of', 'how', 'everywhere', 'been', 'could', 'would', 'others', '’d', 'fifty', 'my', 'part', 'across', 'must', 'anything', 'same', 'anyhow', 'what', 'still', 'without', 'done', 'did', '’s', 'being', 'all', 'doing', 'amongst', 'whose', \"'d\", 'four', 'however', 'often', 'several', 'nor', 'her', 'nine', 'whereupon', 'beforehand', 'for', 'his', 'whither', 'have', 'from', 'amount', 'onto', 'along', 'unless', 'when', 'once', 'off', 'now', 'beyond', 'moreover', 'give', 'since', 'regarding', 'towards', 'during', 'please', 'five', 'she', '’re', 'name'}\n",
      "326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regrex for removing characters\n",
    "import re\n",
    "\n",
    "# Spacy for spell check\n",
    "import spacy\n",
    "import contextualSpellCheck\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "contextualSpellCheck.add_to_pipe(nlp)\n",
    "\n",
    "# Spacy stop word creation\n",
    "stopping_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stopping_words_new = stopping_words\n",
    "# -----------------------------\n",
    "print(stopping_words)\n",
    "print(len(stopping_words))\n",
    "\n",
    "# NLTK for tokenization and lemmatization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'throughout', 'me', 'beside', 'namely', 'go', 'less', 'are', 'everything', 'so', 'sometimes', 'seemed', 'but', 'keep', 'become', 'should', 'cannot', 'is', 'than', 'seems', 'get', 'more', 'can', 'who', 'quite', 'around', 'even', 'might', 'not', 'top', 'neither', 'itself', 'whom', 'any', 'at', 'yourself', 'other', 'hereupon', 'nevertheless', 'wherever', 'via', 'only', 'somewhere', 'perhaps', 'made', 'yet', 'see', '‘ll', '‘m', 'thereafter', 'an', 're', 'where', '’ll', 'here', 'most', 'such', 'bottom', 'there', 'well', '‘ve', 'just', 'about', 'twelve', 'does', 'whereafter', 'eleven', 'else', 'using', 'ourselves', 'on', 'latterly', 'besides', 'that', 'whoever', 'into', 'or', 'sometime', 'becomes', 'had', 'us', 'almost', 'ca', 'someone', 'least', \"'ll\", 'somehow', 'due', 'n’t', 'whereas', 'move', 'hundred', 'again', 'sixty', 'also', 'against', 'thereby', 'serious', 'our', 'afterwards', '’ve', 'anyone', 'rather', 'everyone', 'themselves', 'nowhere', 'anyway', 'above', \"'s\", 'something', 'take', 'he', 'him', 'them', 'to', 'mostly', 'another', 'next', 'latter', 'six', 'their', 'thus', 'am', 'up', 'otherwise', 'may', '‘re', 'mine', 'ever', 'himself', 'upon', 'yourselves', 'therein', 'below', \"n't\", 'forty', 'you', 'as', 'though', 'used', '‘d', 'was', '’m', 'hers', 'fifteen', 'each', 'formerly', 'already', 'your', 'with', 'few', 'the', 'they', 'too', 'seeming', 'and', 'hence', 'some', \"'re\", 'per', 'whereby', 'a', 'be', 'last', 'will', 'down', 'make', 'together', 'between', '‘s', 'one', 'becoming', 'every', 'thereupon', 'front', \"'ve\", 'indeed', 'those', 'herself', 'thru', 'has', 'why', 'whatever', 'whence', 'among', 'through', 'whether', 'hereafter', 'much', 'hereby', 'we', 'noone', 'these', 'toward', 'call', \"'m\", 'put', 'under', 'within', 'ten', 'seem', 'nothing', 'therefore', 'show', 'meanwhile', 'always', 'out', 'nobody', 'thence', 'both', 'herein', 'two', 'after', 'yours', 'then', 'which', 'twenty', 'back', 'first', 'because', 'except', 'own', 'either', 'many', 'really', 'became', 'until', 'ours', 'over', 'while', 'former', 'side', 'behind', 'whole', 'this', 'anywhere', 'n‘t', 'say', 'three', 'its', 'third', 'wherein', 'in', 'very', 'myself', 'further', 'never', 'if', 'it', 'i', 'whenever', 'elsewhere', 'although', 'enough', 'before', 'various', 'do', 'none', 'eight', 'alone', 'empty', 'by', 'were', 'full', 'of', 'how', 'everywhere', 'been', 'could', 'would', 'others', '’d', 'fifty', 'my', 'part', 'across', 'must', 'anything', 'same', 'anyhow', 'what', 'still', 'without', 'done', 'did', '’s', 'being', 'all', 'doing', 'amongst', 'whose', \"'d\", 'four', 'however', 'often', 'several', 'nor', 'her', 'nine', 'whereupon', 'beforehand', 'for', 'his', 'whither', 'have', 'from', 'amount', 'onto', 'along', 'unless', 'when', 'once', 'off', 'now', 'beyond', 'moreover', 'give', 'since', 'regarding', 'towards', 'during', 'please', 'five', 'she', '’re', 'name'}\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "# Removing certain stop words from the list of stop words because they are part of our seed terms\n",
    "stopping_words_to_keep = ['no']\n",
    "for word in list(stopping_words_new):\n",
    "    if word in stopping_words_to_keep:\n",
    "        stopping_words_new.remove(word)\n",
    "print(stopping_words_new)\n",
    "print(len(stopping_words_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of keywords\n",
    "\n",
    "def preprocessing_keywords(raw_string):\n",
    "    \n",
    "    # Remove html tags and anything inside them \n",
    "    no_html = re.sub(r'<[^>]*>','', raw_string)\n",
    "\n",
    "    # Make everything lowercase\n",
    "    lowercase_column = no_html.lower()\n",
    "\n",
    "    # TODO have a look at apostrophe again\n",
    "    # Remove apostrophe \n",
    "    without_apostrophe = re.sub(r'[\\']', '', lowercase_column)\n",
    "    \n",
    "    # Remove all non alphabetic instances that aren't a space and replace them with a space \n",
    "    alphabetic_keywords = re.sub(r'[^a-z\\s]', ' ', without_apostrophe)\n",
    "\n",
    "    # Tokenize string into individual words\n",
    "    tokens = word_tokenize(alphabetic_keywords) \n",
    "    \n",
    "    # Remove stopping words\n",
    "    tokens_without_stopping_words = [token for token in tokens if token not in stopping_words_new]\n",
    "\n",
    "    # Lemmatize tokens using nltk and join them into phrases\n",
    "    sentances_without_stop_words = ' '.join([lemmatizer.lemmatize(t) for t in tokens_without_stopping_words])\n",
    "\n",
    "    return sentances_without_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reusable', 'recycled', 'low impact', 'environmentally friendly', 'cosmos', 'eco', 'recyclable', 'natrue', 'reef safe', 'triclosan free', 'sustainable', 'refill', 'plastic free', 'ecological', 'conservation', 'refillable', 'compostable', 'soil association', 'biodegradable', 'no packaging', 'renewable', 'no oxybenzone', 'fsc', 'organic']\n",
      "['cruelty free', 'ethical', 'donated', 'equality', 'peta', 'no animal', 'leaping bunny', 'fair trade', 'non profit']\n",
      "['recycled', 'economic prosperity', 'local farmer', 'renewable', 'local ingredient', 'small business', 'locally sourced', 'community', 'fair trade', 'fair wage']\n",
      "['natrue', 'phthalates free', 'fragrance free', 'no toxic', 'usda', 'paraben free', 'triclosan free', 'no fragrance', 'non nano', 'cosmos', 'sulfate free', 'formaldehyde free', 'soil association', 'non gmo', 'organic', 'non toxic']\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                      Cleaning Keywords                                \n",
    "\n",
    "#ngram (1,2)\n",
    "enviromental = ['recyclable', 'recycled', ' Environmentally friendly ','biodegradable', ' no packaging', 'sustainable', ' ecological ', 'plastic-free', 'compostable', 'renewable', 'reusable', 'biodegradable', 'organic', 'refillable', 'refills', 'recycled', 'reef safe',  'no oxybenzone ',  'triclosan-free', 'low-impact', 'soil association', 'conservation', 'COSMOS ', 'NATRUE ', 'FSC', 'eco']   \n",
    "\n",
    "\n",
    "social = ['cruelty-free', 'equality', 'PETA', 'leaping bunny', 'fair trade', 'no animal', 'ethical ', ' Non-profit ', ' Donated ']  \n",
    "\n",
    "\n",
    "economic = ['Fair trade', 'renewable', 'locally sourced', 'small business', 'recycled', ' Fair wage', 'community', ' Economic prosperity ', ' local ingredients ', 'local farmers ']   \n",
    "\n",
    "\n",
    "health = ['non-toxic', 'no toxic', 'organic', 'paraben-free', 'triclosan-free', 'phthalates-free', 'non-nano', 'formaldehyde-free', 'non GMO', 'soil association', 'COSMOS ', 'NATRUE', 'USDA', ' Fragrance-free ', ' no fragrance ', ' sulfate free ']   \n",
    "\n",
    "\n",
    "\n",
    "# Cleaning keywords/phrases \n",
    "enviro_p = list(set(preprocessing_keywords(phrase) for phrase in enviromental))\n",
    "social_p = list(set(preprocessing_keywords(phrase) for phrase in social))\n",
    "economic_p = list(set(preprocessing_keywords(phrase) for phrase in economic))\n",
    "health_p = list(set(preprocessing_keywords(phrase) for phrase in health))\n",
    "\n",
    "print(enviro_p)\n",
    "print(social_p)\n",
    "print(economic_p)\n",
    "print(health_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spell check (creating dictionary)\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# term_index is the column of the term and count_index is the column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# Combining all keywords as individual words\n",
    "anchor_words_combined = list(set([word for sublist in [enviro_p, social_p, economic_p, health_p] for phrase in sublist for word in phrase.split()]))\n",
    "\n",
    "# Add all words to the dictionary\n",
    "for word in anchor_words_combined:\n",
    "    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    if len(suggestions) > 0 and suggestions[0].term == word:\n",
    "        # the best suggestion for this word is itself - it must exist in the dictionary\n",
    "        continue\n",
    "\n",
    "    # add to the dictionary with frequency 1 \n",
    "    sym_spell.create_dictionary_entry(word, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of reviews\n",
    "\n",
    "def preprocessing(raw_string):\n",
    "    # Remove html tags and anything inside them \n",
    "    no_html = re.sub(r'<[^>]*>','', raw_string)\n",
    "\n",
    "    # Make everything lowercase\n",
    "    lowercase_column = no_html.lower()\n",
    "    \n",
    "    # Remove all non alphabetic instances that aren't a space and replace them with a space \n",
    "    alphabetic_column = re.sub(r'[^a-z\\s]', ' ', lowercase_column)\n",
    "\n",
    "    # Tokenize string into individual words\n",
    "    tokens = word_tokenize(alphabetic_column) \n",
    "    \n",
    "    # For each word we will have a list of suggestions\n",
    "    spelling_suggestions = [sym_spell.lookup(x, Verbosity.CLOSEST, max_edit_distance=2) for x in tokens]\n",
    "    # Drop those that have no suggestions and top[0] suggestions for those who do\n",
    "    spelling_suggestions = [x[0].term for x in spelling_suggestions if len(x) > 0]\n",
    "    \n",
    "    # Remove stopping words \n",
    "    tokens_without_stopping_words = [token for token in spelling_suggestions if token not in stopping_words_new]\n",
    "\n",
    "    # Lemmatize tokens (nltk)\n",
    "    sentances_without_stop_words = ' '.join([lemmatizer.lemmatize(t) for t in tokens_without_stopping_words])\n",
    "\n",
    "    return sentances_without_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean reviews\n",
    "df[\"clean_reviews\"] = df[\"newReviewText\"].apply(preprocessing)\n",
    "# Export to CSV for sense checking\n",
    "df[\"clean_reviews\"]\n",
    "\n",
    "df[\"clean_reviews\"].to_csv(\"Clean Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any empty reviews or nulls  \n",
    "df = df.drop(df[ ( (df[\"clean_reviews\"] == \"\" ) | ( df[\"clean_reviews\"].isnull()) ) ].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>asin</th>\n",
       "      <th>newReviewText</th>\n",
       "      <th>num_words_reviews</th>\n",
       "      <th>clean_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>My  husband wanted to reading about the Negro ...</td>\n",
       "      <td>0143026860</td>\n",
       "      <td>My  husband wanted to reading about the Negro ...</td>\n",
       "      <td>29</td>\n",
       "      <td>husband wanted reading negro baseball great ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>I am already a baseball fan and knew a bit abo...</td>\n",
       "      <td>0143026860</td>\n",
       "      <td>I am already a baseball fan and knew a bit abo...</td>\n",
       "      <td>23</td>\n",
       "      <td>baseball fan knew bit negro league learned lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This was a good story of the Black leagues. I ...</td>\n",
       "      <td>0143026860</td>\n",
       "      <td>This was a good story of the Black leagues. I ...</td>\n",
       "      <td>67</td>\n",
       "      <td>good story black league bought book teach high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>I didn't like this product it smudged all unde...</td>\n",
       "      <td>014789302X</td>\n",
       "      <td>I didn't like this product it smudged all unde...</td>\n",
       "      <td>14</td>\n",
       "      <td>like product smudged eye thoroughly day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>it burns your eyes when u put it on  and very ...</td>\n",
       "      <td>014789302X</td>\n",
       "      <td>it burns your eyes when u put it on  and very ...</td>\n",
       "      <td>35</td>\n",
       "      <td>burn eye light going forth lot dark eyeliner c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210647</th>\n",
       "      <td>371338</td>\n",
       "      <td>love! love! love! these what a Time Saver for ...</td>\n",
       "      <td>B01HJEGTYK</td>\n",
       "      <td>love! love! love! these what a Time Saver for ...</td>\n",
       "      <td>38</td>\n",
       "      <td>love love love time saver people like hair gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210648</th>\n",
       "      <td>371340</td>\n",
       "      <td>It was awful. It was super frizzy and I tried ...</td>\n",
       "      <td>B01HJEGTYK</td>\n",
       "      <td>It was awful. It was super frizzy and I tried ...</td>\n",
       "      <td>28</td>\n",
       "      <td>awful super frizzy tried comb fell completely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210649</th>\n",
       "      <td>371341</td>\n",
       "      <td>I was skeptical about buying this.  Worried it...</td>\n",
       "      <td>B01HJEGTYK</td>\n",
       "      <td>I was skeptical about buying this.  Worried it...</td>\n",
       "      <td>43</td>\n",
       "      <td>sceptical buying worried look obviously fake s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210650</th>\n",
       "      <td>371343</td>\n",
       "      <td>Way lighter than photo\\nNot mix blend of color...</td>\n",
       "      <td>B01HJEGTYK</td>\n",
       "      <td>Way lighter than photoNot mix blend of colorsN...</td>\n",
       "      <td>25</td>\n",
       "      <td>way lighter photon mix blend quality shown vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210651</th>\n",
       "      <td>371344</td>\n",
       "      <td>No return instructions/phone # in packaging.  ...</td>\n",
       "      <td>B01HJEGTYK</td>\n",
       "      <td>No return instructions/phone # in packaging.  ...</td>\n",
       "      <td>32</td>\n",
       "      <td>no return instruction phone packaging color or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210652 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                         reviewText        asin  \\\n",
       "0            1  My  husband wanted to reading about the Negro ...  0143026860   \n",
       "1            3  I am already a baseball fan and knew a bit abo...  0143026860   \n",
       "2            4  This was a good story of the Black leagues. I ...  0143026860   \n",
       "3            7  I didn't like this product it smudged all unde...  014789302X   \n",
       "4            9  it burns your eyes when u put it on  and very ...  014789302X   \n",
       "...        ...                                                ...         ...   \n",
       "210647  371338  love! love! love! these what a Time Saver for ...  B01HJEGTYK   \n",
       "210648  371340  It was awful. It was super frizzy and I tried ...  B01HJEGTYK   \n",
       "210649  371341  I was skeptical about buying this.  Worried it...  B01HJEGTYK   \n",
       "210650  371343  Way lighter than photo\\nNot mix blend of color...  B01HJEGTYK   \n",
       "210651  371344  No return instructions/phone # in packaging.  ...  B01HJEGTYK   \n",
       "\n",
       "                                            newReviewText  num_words_reviews  \\\n",
       "0       My  husband wanted to reading about the Negro ...                 29   \n",
       "1       I am already a baseball fan and knew a bit abo...                 23   \n",
       "2       This was a good story of the Black leagues. I ...                 67   \n",
       "3       I didn't like this product it smudged all unde...                 14   \n",
       "4       it burns your eyes when u put it on  and very ...                 35   \n",
       "...                                                   ...                ...   \n",
       "210647  love! love! love! these what a Time Saver for ...                 38   \n",
       "210648  It was awful. It was super frizzy and I tried ...                 28   \n",
       "210649  I was skeptical about buying this.  Worried it...                 43   \n",
       "210650  Way lighter than photoNot mix blend of colorsN...                 25   \n",
       "210651  No return instructions/phone # in packaging.  ...                 32   \n",
       "\n",
       "                                            clean_reviews  \n",
       "0       husband wanted reading negro baseball great ad...  \n",
       "1       baseball fan knew bit negro league learned lot...  \n",
       "2       good story black league bought book teach high...  \n",
       "3                 like product smudged eye thoroughly day  \n",
       "4       burn eye light going forth lot dark eyeliner c...  \n",
       "...                                                   ...  \n",
       "210647  love love love time saver people like hair gre...  \n",
       "210648  awful super frizzy tried comb fell completely ...  \n",
       "210649  sceptical buying worried look obviously fake s...  \n",
       "210650  way lighter photon mix blend quality shown vol...  \n",
       "210651  no return instruction phone packaging color or...  \n",
       "\n",
       "[210652 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset index of the data frame for easy use\n",
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1390211\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Setting vectorizer to take phrases in range 1 to 2\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range= (1,2)) \n",
    "\n",
    "# Fit the vectorizer and transform it on the reviews corpus\n",
    "doc_word = vectorizer.fit_transform(df[\"clean_reviews\"])\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Anchor word not in word column labels provided to CorEx: cosmos\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: natrue\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: triclosan free\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: soil association\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: no oxybenzone\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: fsc\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: leaping bunny\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: economic prosperity\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: local ingredient\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: natrue\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: triclosan free\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: cosmos\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: soil association\n",
      "Print topics in text file\n",
      "Theme 1: ['organic', 'refill', 'teeth', 'review', 'water', 'use', 'gum', 'don', 'no', 'time']\n",
      "Theme 2: ['cruelty free', 'no animal', 'look', 'looking', 'donated', 'update', 'cruelty', 'fair trade', 'slightly', 'add']\n",
      "Theme 3: ['razor', 'shave', 'blade', 'shaver', 'morello', 'shaving', 'battery', 'close', 'trimmer', 'electric']\n",
      "Theme 4: ['skin', 'organic', 'smell', 'scent', 'dry', 'face', 'feel', 'product', 'natural', 'cream']\n"
     ]
    }
   ],
   "source": [
    "# Code based on https://github.com/gregversteeg/corex_topic/blob/mastera/corextopic/example/corex_topic_example.ipynb\n",
    "import corextopic.corextopic as ct\n",
    "import corextopic.vis_topic as vt\n",
    "\n",
    "anchor_words = [enviro_p, social_p, economic_p, health_p]\n",
    "\n",
    "anchored_topic_model = ct.Corex(n_hidden=4, seed = 11)\n",
    "anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=11)\n",
    "\n",
    "# Same results of corex model for evaluation\n",
    "vt.vis_rep(anchored_topic_model, column_label=words, prefix='CX Reviews')#FINAL 17-4 Reviews CorEx Model\n",
    "\n",
    "topics_list = anchored_topic_model.get_topics()\n",
    "\n",
    "# Get top 10 phrases for each topic\n",
    "top_words = []\n",
    "for sublist in topics_list:\n",
    "    phrase = [item[0] for item in sublist]\n",
    "    top_words.append(phrase)\n",
    "\n",
    "for index, phrase in enumerate(top_words):\n",
    "    print(f\"Theme {index+1}:\", phrase)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1390211\n"
     ]
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>     GLDA VECTORIZATION         <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Setting vectorizer to take phrases in range 1 to 2 (non binary for GLDA)\n",
    "vectorizer_GLDA = CountVectorizer(binary=False, ngram_range= (1,2))\n",
    "\n",
    "# Fit the vectorizer and transform & fit it on the description corpus\n",
    "vectorised_descriptions_corpus = vectorizer_GLDA.fit_transform(df[\"clean_reviews\"])\n",
    "# TODO COMMENT\n",
    "word2id = vectorizer_GLDA.vocabulary_ #columns dictionary\n",
    "\n",
    "vocab_GLDA = vectorizer_GLDA.get_feature_names_out() \n",
    "print(len(vocab_GLDA))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['reusable', 'recycled', 'low impact', 'environmentally friendly', 'eco', 'recyclable', 'reef safe', 'sustainable', 'refill', 'plastic free', 'ecological', 'conservation', 'refillable', 'compostable', 'biodegradable', 'no packaging', 'renewable', 'organic'], ['cruelty free', 'ethical', 'donated', 'equality', 'peta', 'no animal', 'fair trade', 'non profit'], ['recycled', 'local farmer', 'renewable', 'small business', 'locally sourced', 'community', 'fair trade', 'fair wage'], ['phthalates free', 'fragrance free', 'no toxic', 'usda', 'paraben free', 'no fragrance', 'non nano', 'sulfate free', 'formaldehyde free', 'non gmo', 'organic', 'non toxic']]\n",
      "------------------\n",
      "[['reusable', 'recycled', 'low impact', 'environmentally friendly', 'eco', 'recyclable', 'reef safe', 'sustainable', 'refill', 'plastic free', 'ecological', 'conservation', 'refillable', 'compostable', 'biodegradable', 'no packaging', 'renewable', 'organic'], ['cruelty free', 'ethical', 'donated', 'equality', 'peta', 'no animal', 'fair trade', 'non profit'], ['recycled', 'local farmer', 'renewable', 'small business', 'locally sourced', 'community', 'fair trade', 'fair wage'], ['phthalates free', 'fragrance free', 'no toxic', 'usda', 'paraben free', 'no fragrance', 'non nano', 'sulfate free', 'formaldehyde free', 'non gmo', 'organic', 'non toxic']]\n"
     ]
    }
   ],
   "source": [
    "anchor_words = [enviro_p, social_p, economic_p, health_p]\n",
    "print(anchor_words)\n",
    "\n",
    "achors_to_remove =  [\"cosmos\", \"natrue\", \"triclosan free\", \"soil association\", \"no oxybenzone\", \"fsc\", \"leaping bunny\",\"economic prosperity\", \"local ingredient\"]\n",
    "\n",
    "#enviro_to_remove = [\"cosmos\", \"natrue\", \"triclosan free\", \"soil association\", \"no oxybenzone\", \"fsc\"]\n",
    "#social_to_remove = [\"leaping bunny\"]\n",
    "#economic_to_remove = [\"economic prosperity\", \"local ingredient\"]\n",
    "#health_to_remove = [\"natrue\", \"triclosan free\", \"cosmos\", \"soil association\"]\n",
    "\n",
    "for word in achors_to_remove:\n",
    "    for i in range(4):\n",
    "        if word in anchor_words[i]:\n",
    "            anchor_words[i].remove(word)\n",
    "        \n",
    "print(\"------------------\")\n",
    "print(anchor_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 210652\n",
      "INFO:lda:vocab_size: 1390211\n",
      "INFO:lda:n_words: 8054004\n",
      "INFO:lda:n_topics: 4\n",
      "INFO:lda:n_iter: 2000\n",
      "INFO:lda:<0> log likelihood: -100424456\n",
      "INFO:lda:<50> log likelihood: -92426867\n",
      "INFO:lda:<100> log likelihood: -91946922\n",
      "INFO:lda:<150> log likelihood: -91842709\n",
      "INFO:lda:<200> log likelihood: -91819648\n",
      "INFO:lda:<250> log likelihood: -91799651\n",
      "INFO:lda:<300> log likelihood: -91794569\n",
      "INFO:lda:<350> log likelihood: -91773638\n",
      "INFO:lda:<400> log likelihood: -91756146\n",
      "INFO:lda:<450> log likelihood: -91748331\n",
      "INFO:lda:<500> log likelihood: -91741842\n",
      "INFO:lda:<550> log likelihood: -91744843\n",
      "INFO:lda:<600> log likelihood: -91744097\n",
      "INFO:lda:<650> log likelihood: -91747044\n",
      "INFO:lda:<700> log likelihood: -91740703\n",
      "INFO:lda:<750> log likelihood: -91748869\n",
      "INFO:lda:<800> log likelihood: -91741702\n",
      "INFO:lda:<850> log likelihood: -91750023\n",
      "INFO:lda:<900> log likelihood: -91751218\n",
      "INFO:lda:<950> log likelihood: -91742182\n",
      "INFO:lda:<1000> log likelihood: -91747855\n",
      "INFO:lda:<1050> log likelihood: -91747011\n",
      "INFO:lda:<1100> log likelihood: -91756856\n",
      "INFO:lda:<1150> log likelihood: -91747715\n",
      "INFO:lda:<1200> log likelihood: -91744215\n",
      "INFO:lda:<1250> log likelihood: -91753735\n",
      "INFO:lda:<1300> log likelihood: -91742935\n",
      "INFO:lda:<1350> log likelihood: -91736112\n",
      "INFO:lda:<1400> log likelihood: -91733734\n",
      "INFO:lda:<1450> log likelihood: -91741849\n",
      "INFO:lda:<1500> log likelihood: -91726718\n",
      "INFO:lda:<1550> log likelihood: -91734264\n",
      "INFO:lda:<1600> log likelihood: -91738898\n",
      "INFO:lda:<1650> log likelihood: -91737183\n",
      "INFO:lda:<1700> log likelihood: -91730592\n",
      "INFO:lda:<1750> log likelihood: -91734434\n",
      "INFO:lda:<1800> log likelihood: -91737272\n",
      "INFO:lda:<1850> log likelihood: -91729903\n",
      "INFO:lda:<1900> log likelihood: -91731339\n",
      "INFO:lda:<1950> log likelihood: -91735217\n",
      "INFO:lda:<1999> log likelihood: -91733913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: color, great, look, like, brush, little, nice, love, price, quality\n",
      "Topic 1: product, hair, work, use, don, time, day, week, no, review\n",
      "Topic 2: good, year, great, razor, bought, shave, time, easy, better, no\n",
      "Topic 3: skin, love, product, smell, like, feel, use, face, dry, great\n"
     ]
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>      Guided - LDA          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# Based on https://www.kaggle.com/code/nvpsani/topic-modelling-using-guided-lda who used workaround GLDA as well\n",
    "from lda import guidedlda as glda\n",
    "import numpy as np\n",
    "\n",
    "# Defining model\n",
    "model = glda.GuidedLDA(n_topics=4, alpha=12.5, eta=0.1, n_iter=2000, random_state=99, refresh=50)\n",
    "\n",
    "# Topics for the model: create a mapping from feature column index to topic ID\n",
    "anchor_topics = {}\n",
    "for topic_id in range(len(anchor_words)):\n",
    "    key_word_list = anchor_words[topic_id]\n",
    "    for word in key_word_list:\n",
    "        col_index = word2id[word]\n",
    "        anchor_topics[col_index] = topic_id\n",
    "\n",
    "# Train model\n",
    "model.fit(vectorised_descriptions_corpus, seed_topics=anchor_topics, seed_confidence=1.1)\n",
    "\n",
    "# Extract the importance of each feature towards each topic, and display the top ones\n",
    "NUM_TOP_FEATURES = 10\n",
    "words_in_topics = model.topic_word_\n",
    "for topic_id in range(len(words_in_topics)):\n",
    "    # Get the feature importance for this one topic\n",
    "    feature_importance = words_in_topics[topic_id]\n",
    "    # Get the indices of the list when sorted\n",
    "    sorted_indices = np.argsort(feature_importance)\n",
    "    # Reverse to get most important first, and take the most important ones\n",
    "    important_features_indices = sorted_indices[::-1][:NUM_TOP_FEATURES]\n",
    "    # Use our vocab list to map to the feature names\n",
    "    top_words = np.array(vocab_GLDA)[important_features_indices]\n",
    "    \n",
    "    print(f'Topic {topic_id}: {\", \".join(top_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34136359 0.32246684 0.2077321  0.12843747]\n",
      " [0.47343705 0.26878913 0.03333832 0.2244355 ]\n",
      " [0.34026852 0.28293264 0.15424667 0.22255218]\n",
      " ...\n",
      " [0.51445361 0.37561892 0.03683785 0.07308961]\n",
      " [0.51663265 0.21402148 0.0302978  0.23904806]\n",
      " [0.45747977 0.18157116 0.26995996 0.09098911]]\n",
      "[[0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 0]\n",
      " ...\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Transform the model to be able to set a threshold and get document topic lables\n",
    "doc_topic = model.transform(vectorised_descriptions_corpus)\n",
    "print(doc_topic)\n",
    "\n",
    "# Set threshold\n",
    "threshold = 0.45\n",
    "\n",
    "doc_topic_thresholded = (doc_topic >= threshold).astype(int)\n",
    "print(doc_topic_thresholded)\n",
    "\n",
    "\n",
    "# Output it\n",
    "df_glda_labels = pd.DataFrame(doc_topic_thresholded)\n",
    "df_glda_labels.to_csv('FINAL 17-4 Reviews GLDA Model.txt', sep='\\t', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
