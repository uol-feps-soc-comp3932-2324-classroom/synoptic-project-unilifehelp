{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21836\\2769020576.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Importing metadata zip file and converting it to dataframe\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "def getDF(path):\n",
    "  # Unzip the file, load in each line as an object\n",
    "  g = gzip.open(path, 'rb')\n",
    "  g = [json.loads(l) for l in g]\n",
    "\n",
    "  # Map to a dictionary, then load in as a dataframe\n",
    "  dict_df = {i: d for (i, d) in enumerate(g)}\n",
    "  return pd.DataFrame.from_dict(dict_df, orient='index')\n",
    "\n",
    "df = getDF('meta_ALL_Beauty.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32892</td>\n",
       "      <td>32892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13751</td>\n",
       "      <td>32488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[]</td>\n",
       "      <td>B00027CDOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>17773</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       description        asin\n",
       "count        32892       32892\n",
       "unique       13751       32488\n",
       "top             []  B00027CDOW\n",
       "freq         17773           2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Selecting only the colums that are required for analysis\n",
    "\n",
    "colums_description_asin = [\"description\",\"asin\"]\n",
    "df = df[colums_description_asin]\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptions in the json file are a list of strings, converting to sting for easy cleaning\n",
    "df[\"newdescription\"] = df.description.map(lambda x: \".\".join(x).replace(\"\\n\",\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty descriptions\n",
    "#df = df.drop(df[df[\"newdescription\"] == \"\"].index)\n",
    "\n",
    "#print(len(df)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>asin</th>\n",
       "      <th>newdescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Loud 'N Clear Personal Sound Amplifier allows...</td>\n",
       "      <td>6546546450</td>\n",
       "      <td>Loud 'N Clear Personal Sound Amplifier allows ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[No7 Lift &amp; Luminate Triple Action Serum 50ml ...</td>\n",
       "      <td>7178680776</td>\n",
       "      <td>No7 Lift &amp; Luminate Triple Action Serum 50ml b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[No7 Stay Perfect Foundation now stays perfect...</td>\n",
       "      <td>7250468162</td>\n",
       "      <td>No7 Stay Perfect Foundation now stays perfect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Lacto Calamine Skin Balance Daily Nourishing ...</td>\n",
       "      <td>7414204790</td>\n",
       "      <td>Lacto Calamine Skin Balance Daily Nourishing L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Mary Kay Satin Hands Peach Hand Cream Travel ...</td>\n",
       "      <td>7535842801</td>\n",
       "      <td>Mary Kay Satin Hands Peach Hand Cream Travel S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32880</th>\n",
       "      <td>[Move over soap on a rope! This heavy-duty Bri...</td>\n",
       "      <td>B01HIHLFOC</td>\n",
       "      <td>Move over soap on a rope! This heavy-duty Bric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32884</th>\n",
       "      <td>[Eau de parfum spray vial mini design house: y...</td>\n",
       "      <td>B01HIPOQ2M</td>\n",
       "      <td>Eau de parfum spray vial mini design house: yv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32885</th>\n",
       "      <td>[Pokemon Plush 9.2 Inch / 23cm Gengar Doll Stu...</td>\n",
       "      <td>B01HIUEEHO</td>\n",
       "      <td>Pokemon Plush 9.2 Inch / 23cm Gengar Doll Stuf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32886</th>\n",
       "      <td>[New and unused product. 100% authentic Benefi...</td>\n",
       "      <td>B01HIWKGOM</td>\n",
       "      <td>New and unused product. 100% authentic Benefit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32890</th>\n",
       "      <td>[Brand new and high quality&lt;br&gt; Enables fast v...</td>\n",
       "      <td>B01HJASD20</td>\n",
       "      <td>Brand new and high quality&lt;br&gt; Enables fast vo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description        asin  \\\n",
       "0      [Loud 'N Clear Personal Sound Amplifier allows...  6546546450   \n",
       "1      [No7 Lift & Luminate Triple Action Serum 50ml ...  7178680776   \n",
       "2      [No7 Stay Perfect Foundation now stays perfect...  7250468162   \n",
       "4      [Lacto Calamine Skin Balance Daily Nourishing ...  7414204790   \n",
       "5      [Mary Kay Satin Hands Peach Hand Cream Travel ...  7535842801   \n",
       "...                                                  ...         ...   \n",
       "32880  [Move over soap on a rope! This heavy-duty Bri...  B01HIHLFOC   \n",
       "32884  [Eau de parfum spray vial mini design house: y...  B01HIPOQ2M   \n",
       "32885  [Pokemon Plush 9.2 Inch / 23cm Gengar Doll Stu...  B01HIUEEHO   \n",
       "32886  [New and unused product. 100% authentic Benefi...  B01HIWKGOM   \n",
       "32890  [Brand new and high quality<br> Enables fast v...  B01HJASD20   \n",
       "\n",
       "                                          newdescription  \n",
       "0      Loud 'N Clear Personal Sound Amplifier allows ...  \n",
       "1      No7 Lift & Luminate Triple Action Serum 50ml b...  \n",
       "2      No7 Stay Perfect Foundation now stays perfect ...  \n",
       "4      Lacto Calamine Skin Balance Daily Nourishing L...  \n",
       "5      Mary Kay Satin Hands Peach Hand Cream Travel S...  \n",
       "...                                                  ...  \n",
       "32880  Move over soap on a rope! This heavy-duty Bric...  \n",
       "32884  Eau de parfum spray vial mini design house: yv...  \n",
       "32885  Pokemon Plush 9.2 Inch / 23cm Gengar Doll Stuf...  \n",
       "32886  New and unused product. 100% authentic Benefit...  \n",
       "32890  Brand new and high quality<br> Enables fast vo...  \n",
       "\n",
       "[15108 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df[ ( (df[\"newdescription\"] == \"\" ) | ( df[\"newdescription\"].isnull()) ) ].index)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>asin</th>\n",
       "      <th>newdescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14821</td>\n",
       "      <td>14821</td>\n",
       "      <td>14821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13749</td>\n",
       "      <td>14821</td>\n",
       "      <td>13743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[For over 60 years, Betty Dain Creations, Inc....</td>\n",
       "      <td>6546546450</td>\n",
       "      <td>For over 60 years, Betty Dain Creations, Inc. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description        asin  \\\n",
       "count                                               14821       14821   \n",
       "unique                                              13749       14821   \n",
       "top     [For over 60 years, Betty Dain Creations, Inc....  6546546450   \n",
       "freq                                                   59           1   \n",
       "\n",
       "                                           newdescription  \n",
       "count                                               14821  \n",
       "unique                                              13743  \n",
       "top     For over 60 years, Betty Dain Creations, Inc. ...  \n",
       "freq                                                   59  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate asin so can have only unique products\n",
    "df[df.duplicated(\"asin\")]\n",
    "df = df.drop_duplicates(\"asin\", keep=\"last\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>asin</th>\n",
       "      <th>newdescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13743</td>\n",
       "      <td>13743</td>\n",
       "      <td>13743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13743</td>\n",
       "      <td>13743</td>\n",
       "      <td>13743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[Loud 'N Clear Personal Sound Amplifier allows...</td>\n",
       "      <td>6546546450</td>\n",
       "      <td>Loud 'N Clear Personal Sound Amplifier allows ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description        asin  \\\n",
       "count                                               13743       13743   \n",
       "unique                                              13743       13743   \n",
       "top     [Loud 'N Clear Personal Sound Amplifier allows...  6546546450   \n",
       "freq                                                    1           1   \n",
       "\n",
       "                                           newdescription  \n",
       "count                                               13743  \n",
       "unique                                              13743  \n",
       "top     Loud 'N Clear Personal Sound Amplifier allows ...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate description\n",
    "df[df.duplicated(\"newdescription\")]\n",
    "df = df.drop_duplicates(\"newdescription\", keep=\"last\")\n",
    "df.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newdescription</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13743</td>\n",
       "      <td>13743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13743</td>\n",
       "      <td>13743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loud 'N Clear Personal Sound Amplifier allows ...</td>\n",
       "      <td>6546546450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           newdescription        asin\n",
       "count                                               13743       13743\n",
       "unique                                              13743       13743\n",
       "top     Loud 'N Clear Personal Sound Amplifier allows ...  6546546450\n",
       "freq                                                    1           1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing redundant old description column\n",
    "colums_description_asin = [\"newdescription\",\"asin\"]\n",
    "df = df[colums_description_asin]\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    13743.000000\n",
       "mean        63.643600\n",
       "std         91.940065\n",
       "min          1.000000\n",
       "10%          7.000000\n",
       "15%          9.000000\n",
       "20%         11.000000\n",
       "25%         14.000000\n",
       "30%         17.000000\n",
       "50%         39.000000\n",
       "75%         79.000000\n",
       "85%        114.000000\n",
       "90%        148.000000\n",
       "92%        169.000000\n",
       "95%        217.000000\n",
       "97%        259.000000\n",
       "98%        288.000000\n",
       "99%        324.000000\n",
       "max       3224.000000\n",
       "Name: newdescription, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify range of descriptions in after duplicates removal\n",
    "\n",
    "df_descriptions_without_empty = df[\"newdescription\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "df_descriptions_without_empty.describe([0.1,0.15,0.20,0.25,0.30,0.75,0.85,0.90,0.92,0.95,0.97,0.98,0.99])\n",
    "\n",
    "# 97% upper limit of 259 (same % milit as reviews)\n",
    "# 11 words (same numbe of words as reviews) 20% because no assuption is made on how much more each one is informative as well as descriptions \n",
    "# being a smaller sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 10710\n",
       "unique                                                10710\n",
       "top       Loud 'N Clear Personal Sound Amplifier allows ...\n",
       "freq                                                      1\n",
       "Name: newdescription, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove descriptions with more or less than xx pre-cleaned words\n",
    "\n",
    "# Split at any white space \n",
    "df[\"num_words_description\"] = df[\"newdescription\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Check if under or equal to 80% upper limmit words fulfils withs condition and set it\n",
    "df = df[(df[\"num_words_description\"] <= 259) & (df[\"num_words_description\"] >= 11)]\n",
    "\n",
    "df[\"newdescription\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10710.000000\n",
       "mean        65.241923\n",
       "std         53.053433\n",
       "min         11.000000\n",
       "10%         15.000000\n",
       "15%         18.000000\n",
       "20%         23.000000\n",
       "25%         27.000000\n",
       "30%         31.000000\n",
       "50%         49.000000\n",
       "75%         86.000000\n",
       "85%        115.000000\n",
       "90%        142.000000\n",
       "92%        157.000000\n",
       "95%        185.000000\n",
       "97%        211.000000\n",
       "98%        225.000000\n",
       "99%        243.000000\n",
       "max        259.000000\n",
       "Name: newdescription, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shortening worked\n",
    "df_descriptions_without_empty = df[\"newdescription\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "df_descriptions_without_empty.describe([0.1,0.15,0.20,0.25,0.30,0.75,0.85,0.90,0.92,0.95,0.97,0.98,0.99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Regrex for character removal\n",
    "import re\n",
    "\n",
    "# Spacy for spell check\n",
    "import spacy\n",
    "import contextualSpellCheck\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "contextualSpellCheck.add_to_pipe(nlp)\n",
    "\n",
    "# NLTK for tokenisation and lemmatization\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Spacy stop word creation\n",
    "stopping_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stopping_words_new = stopping_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'among', 'seeming', 'any', 'if', 'whenever', 'they', 'anyone', 'together', 'twenty', 'somewhere', 'whether', 'full', 'keep', 'been', 'wherein', 'go', 'anyway', '’ve', 'front', 'a', 'am', 'herein', 'has', 'behind', 'her', 'can', 'much', 'herself', 'were', 'afterwards', 'hundred', 'these', 'enough', 'most', 'throughout', 'here', 'fifty', 'well', 'even', 'thereby', 'somehow', 'whereafter', 'noone', 'forty', 'never', 'ever', 'hereby', 'four', 'hers', 'ten', 'seems', 'side', 'within', 'many', 'themselves', \"'d\", '‘d', 'whereas', 'just', 'nothing', 'already', 'see', 'is', 'several', 'on', 'across', 'very', 'almost', 're', 'another', 'more', 'whatever', 'through', 'still', '‘s', 'mostly', 'himself', 'this', 'nor', 'must', 'doing', 'per', 'our', 'own', 'move', 'thereupon', 'once', 'onto', 'his', 'she', 'everyone', 'n‘t', 'your', 'there', 'nevertheless', 'due', 'again', 'he', 'therefore', 'whereupon', \"'re\", 'latter', 'with', 'twelve', 'yours', '’d', 'else', 'back', 'might', 'whither', 'be', 'top', 'from', 'toward', 'it', 'now', 'what', 'become', 'fifteen', 'every', 'until', 'while', 'amongst', 'sixty', 'also', 'everything', 'that', 'quite', 'perhaps', '’ll', 'six', 'should', 'anything', 'take', 'hereafter', 'him', '‘m', 'various', 'could', 'ca', 'against', 'eight', 'other', 'whole', 'during', 'which', 'hence', 'elsewhere', 'whom', 'where', 'though', 'eleven', 'used', 'into', 'please', 'serious', 'often', 'each', 'my', 'may', 'part', 'upon', 'rather', 'indeed', 'seemed', 'therein', 'between', 'do', 'along', 'had', 'call', 'thru', 'under', 'amount', 'give', 'before', 'thus', 'became', 'thereafter', 'we', 'bottom', 'up', 'whoever', 'however', 'although', 'after', 'beside', 'beyond', 'further', 'or', 'as', 'via', 'did', '’m', 'five', 'something', 'done', 'us', 'about', 'them', 'down', \"'ll\", 'when', 'how', 'at', 'everywhere', \"n't\", 'make', 'are', 'itself', 'whose', 'such', 'nine', 'unless', 'regarding', 'using', '’s', 'few', 'for', 'least', 'wherever', 'nobody', 'becomes', 'becoming', 'ours', 'towards', \"'ve\", 'i', 'out', 'both', '‘ll', 'besides', 'and', 'moreover', 'next', 'below', 'off', 'former', 'three', 'me', 'someone', 'above', 'of', 'an', 'its', 'one', 'sometimes', 'neither', 'namely', 'put', 'yet', 'otherwise', 'to', 'name', 'than', 'so', 'their', '‘ve', 'in', 'others', 'only', 'get', 'really', 'made', 'sometime', 'two', 'because', 'the', 'last', 'will', 'empty', \"'s\", 'too', 'but', 'who', 'hereupon', 'alone', 'then', 'meanwhile', 'say', 'beforehand', 'always', 'yourself', 'seem', 'being', 'around', 'thence', 'same', 'formerly', 'whence', '‘re', 'anyhow', 'n’t', 'yourselves', 'all', 'since', 'was', 'mine', 'latterly', 'why', 'those', 'by', 'first', 'whereby', 'show', 'except', 'you', 'would', 'have', 'does', 'ourselves', 'cannot', 'anywhere', 'myself', 'nowhere', 'third', 'either', 'some', '’re', 'over', \"'m\"}\n"
     ]
    }
   ],
   "source": [
    "stopping_words_to_remove = ['without' , 'not', 'less', 'noting', 'none','no']\n",
    "for word in list(stopping_words_new):\n",
    "    if word in stopping_words_to_remove:\n",
    "        stopping_words_new.remove(word)\n",
    "print(stopping_words_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of descriptions\n",
    "\n",
    "def preprocessing(raw_string):\n",
    "    # Remove html tags and anything inside them \n",
    "    no_html = re.sub(r'<[^>]*>','', raw_string)\n",
    "    #print(\"after removing html\", no_html)\n",
    "\n",
    "\n",
    "    # Make everything lowercase\n",
    "    lowercase_column = no_html.lower()\n",
    "    #print(\"lowercase\", lowercase_column)\n",
    "\n",
    "    # Remove apostrophe to enable spell check to correct words with apostrophe\n",
    "    without_apostrophe = re.sub(r'[\\']', '', lowercase_column)\n",
    "\n",
    "    # ! Need to double check again where best to use this spell check\n",
    "    # \n",
    "    # .pipe for batches of text\n",
    "    #doc = list(nlp.pipe(without_apostrophe))\n",
    "    #doc = nlp(without_apostrophe)\n",
    "\n",
    "    #spell_checked = doc._.outcome_spellCheck\n",
    "\n",
    "    # Remove all non alphabetic instances that aren't a space and replace them with a space using Regrex\n",
    "    alphabetic_column = re.sub(r'[^a-z\\s]', ' ', without_apostrophe)\n",
    "    #print(\"removed numerical and punctuation\", alphabetic_column)\n",
    "\n",
    "    # Tokenize string into individual words\n",
    "    tokens = word_tokenize(alphabetic_column)\n",
    "\n",
    "    # Remove stopping words using Spacy library\n",
    "    tokens_without_stopping_words = [token for token in tokens if token not in stopping_words_new]\n",
    "\n",
    "    # Lemmatize tokens using nltk and join them into sentances\n",
    "    sentances_without_stop_words = ' '.join([lemmatizer.lemmatize(t) for t in tokens_without_stopping_words])\n",
    "\n",
    "    return sentances_without_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        loud n clear personal sound amplifier allows t...\n",
       "2        no stay perfect foundation stay perfect longer...\n",
       "4        lacto calamine skin balance daily nourishing l...\n",
       "5        mary kay satin hand peach hand cream travel si...\n",
       "7        according legend brother native origin black b...\n",
       "                               ...                        \n",
       "32879                                                  NaN\n",
       "32880                                                  NaN\n",
       "32884                                                  NaN\n",
       "32885                                                  NaN\n",
       "32890                                                  NaN\n",
       "Name: clean_description, Length: 10710, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_description\"] = df[\"newdescription\"].head(1000).apply(preprocessing)\n",
    "\n",
    "df[\"clean_description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any empty descriptions that appear because of head()\n",
    "df = df.drop(df[ ( (df[\"clean_description\"] == \"\" ) | ( df[\"clean_description\"].isnull()) ) ].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.00000\n",
       "mean       44.84100\n",
       "std        32.72263\n",
       "min         4.00000\n",
       "3%          9.00000\n",
       "10%        13.00000\n",
       "20%        19.00000\n",
       "30%        24.70000\n",
       "50%        35.00000\n",
       "75%        59.00000\n",
       "85%        76.15000\n",
       "90%        92.00000\n",
       "95%       114.00000\n",
       "max       221.00000\n",
       "Name: clean_description, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying range of descriptions in after cleaning\n",
    " \n",
    "df_description_without_empty_clean = df[\"clean_description\"].apply(lambda x: len(x.split()))\n",
    "#df_descriptions_without_empty.describe()\n",
    "df_description_without_empty_clean.describe([0.03,0.1,0.2,0.3,0.75,0.85,0.90,0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('skin', 762),\n",
       " ('hair', 372),\n",
       " ('oil', 288),\n",
       " ('use', 270),\n",
       " ('natural', 268),\n",
       " ('product', 266),\n",
       " ('body', 229),\n",
       " ('fragrance', 214),\n",
       " ('help', 207),\n",
       " ('not', 178),\n",
       " ('oz', 174),\n",
       " ('clean', 171),\n",
       " ('free', 170),\n",
       " ('formula', 162),\n",
       " ('shave', 156),\n",
       " ('dry', 151),\n",
       " ('no', 146),\n",
       " ('color', 141),\n",
       " ('smooth', 135),\n",
       " ('soft', 134),\n",
       " ('x', 130),\n",
       " ('day', 129),\n",
       " ('ingredient', 128),\n",
       " ('vitamin', 126),\n",
       " ('time', 117),\n",
       " ('blend', 115),\n",
       " ('soap', 113),\n",
       " ('long', 111),\n",
       " ('scent', 110),\n",
       " ('system', 109),\n",
       " ('size', 107),\n",
       " ('water', 107),\n",
       " ('shaving', 106),\n",
       " ('head', 105),\n",
       " ('blade', 103),\n",
       " ('extract', 102),\n",
       " ('line', 99),\n",
       " ('face', 97),\n",
       " ('easy', 97),\n",
       " ('contains', 96),\n",
       " ('nbsp', 96),\n",
       " ('organic', 94),\n",
       " ('bath', 94),\n",
       " ('provides', 93),\n",
       " ('hand', 92),\n",
       " ('designed', 92),\n",
       " ('razor', 91),\n",
       " ('cream', 89),\n",
       " ('note', 87),\n",
       " ('lip', 86),\n",
       " ('e', 84),\n",
       " ('essential', 84),\n",
       " ('woman', 82),\n",
       " ('work', 82),\n",
       " ('perfect', 81),\n",
       " ('area', 81),\n",
       " ('amp', 81),\n",
       " ('without', 80),\n",
       " ('new', 79),\n",
       " ('trimmer', 79),\n",
       " ('gel', 79),\n",
       " ('xl', 79),\n",
       " ('lotion', 77),\n",
       " ('remove', 77),\n",
       " ('minute', 75),\n",
       " ('gently', 75),\n",
       " ('wear', 74),\n",
       " ('design', 74),\n",
       " ('shaver', 74),\n",
       " ('great', 74),\n",
       " ('light', 74),\n",
       " ('feature', 73),\n",
       " ('gentle', 73),\n",
       " ('fresh', 72),\n",
       " ('moisture', 72),\n",
       " ('year', 71),\n",
       " ('leaf', 71),\n",
       " ('rich', 70),\n",
       " ('pure', 70),\n",
       " ('shine', 68),\n",
       " ('treatment', 68),\n",
       " ('brush', 68),\n",
       " ('healthy', 68),\n",
       " ('eye', 68),\n",
       " ('fit', 67),\n",
       " ('sensitive', 66),\n",
       " ('comfortable', 65),\n",
       " ('spray', 65),\n",
       " ('tea', 65),\n",
       " ('quality', 64),\n",
       " ('come', 64),\n",
       " ('apply', 63),\n",
       " ('non', 62),\n",
       " ('feel', 62),\n",
       " ('need', 61),\n",
       " ('unique', 60),\n",
       " ('way', 60),\n",
       " ('protection', 60),\n",
       " ('looking', 60),\n",
       " ('care', 60)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(\" \".join(df[\"clean_description\"]).split()).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biodegradable', 'reduced packaging', 'reduced', 'sustainable', 'plastic free', 'sustainably sourced', 'compostable', 'renewable', 'renewable energy', 'reusable', 'biodegradable', 'organic', 'refillable', 'refill', 'solid bar', 'recycled', 'cardboard', 'reef safe', 'reef friendly', 'oxybenzone free', 'triclosan free', 'microplastics free', 'microbeads free', 'palm oil free', 'hdpe', 'post consumer recycled plastic', 'renewable energy', 'recycling scheme', 'sustainably sourced', 'low impact', 'carbon neutral', 'carbon offsetting', 'eco', 'soil association', 'conservation', 'cosmos', 'natrue', 'rspo', 'fsc']\n",
      "['no animal testing', 'cruelty free', 'vegan', 'plant based', 'palm oil free', 'ethical', 'vegan society', 'peta', 'leaping bunny', 'fair trade', 'local', 'hand', 'small business']\n",
      "['fair trade', 'renewable energy', 'circular economy', 'locally sourced', 'local', 'small business', 'job creation']\n",
      "['non toxic', 'bio', 'organic', 'plant based', 'paraben free', 'triclosan free', 'fragrance free', 'synthetic fragrance free', 'sl free', 'phthalates free', 'nanoparticles free', 'non nano', 'formaldehyde free', 'phthalates free', 'no gmo', 'soil association', 'cosmos', 'natrue', 'usda']\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                      Cleaning Keywords                                \n",
    "\n",
    "# TODO use fanilized list of keywords\n",
    "\n",
    "enviromental = ['biodegradable', 'reduced packaging', 'reduced', 'sustainable', 'plastic-free', 'sustainably sourced', 'compostable', 'renewable', 'renewable energy', 'reusable', 'biodegradable', 'organic', 'refillable', 'refills', 'solid bar', 'recycled', 'cardboard', 'reef safe','reef-friendly', 'oxybenzone free', 'triclosan-free', 'microplastics free', 'microbeads free', 'palm oil free', 'HDPE', 'post-consumer recycled plastic', 'renewable energy', 'recycling scheme', 'sustainably sourced', 'low-impact', 'carbon neutral', 'carbon offsetting', 'eco', 'soil association', 'conservation', 'COSMOS', 'NATRUE', 'RSPO', 'FSC']\n",
    "social = ['No animal testing', 'cruelty-free', 'vegan', 'plant-based', 'palm oil-free', 'ethical', 'vegan society', 'PETA', 'leaping bunny', 'fair trade', 'local', 'hand-made', 'small business'] \n",
    "economic = ['Fair trade', 'renewable energy', 'circular economy', 'locally sourced', 'local', 'small business', 'job creation']\n",
    "health = ['non-toxic', 'bio', 'organic', 'plant-based', 'paraben free', 'triclosan-free', 'fragrance-free', 'synthetic fragrance-free', 'SLS free', 'phthalates free','nanoparticles free', 'non-nano', 'formaldehyde free', 'phthalates free', 'no GMO', 'soil association', 'COSMOS', 'NATRUE', 'USDA']\n",
    "\n",
    "# Cleaning keywords in the same way as the corpus\n",
    "clean_enviro = list(map(preprocessing, enviromental))\n",
    "clean_social = list(map(preprocessing, social))\n",
    "clean_economic = list(map(preprocessing, economic))\n",
    "clean_health = list(map(preprocessing, health))\n",
    "\n",
    "print(clean_enviro)\n",
    "print(clean_social)\n",
    "print(clean_economic)\n",
    "print(clean_health)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7433)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################\n",
    "#                      CorEx                                  #\n",
    "# Code based on\n",
    "# https://github.com/gregversteeg/corex_topic/blob/master/corextopic/example/corex_topic_example.ipynb\n",
    "\n",
    "import corextopic.corextopic as ct\n",
    "\n",
    "# Setting anchor words for corEx\n",
    "# Anchor with group of words\n",
    "anchor_words = [clean_enviro, clean_social, clean_economic, clean_health]\n",
    "\n",
    "#------------------------------------------------------\n",
    "# fow working with sparse arrays\n",
    "import scipy.sparse as ss\n",
    "\n",
    "# Vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorise the dataset\n",
    "\n",
    "\n",
    "# TODO double check which stopwords removed no need for stop_words='englsih' because those are removed already \n",
    "# TODO check if max_features=20000 is needed, if there is a point to use it as it will select top words from everything, number might need adjusting\n",
    "# TODO invest9gate the chance of using Max_df to remove some words with high frequency\n",
    "# binary set to True because this takes frequesncy of each word into account (for GLDA this need to be set to default false as it only accepts 0 or 1)\n",
    "vectorizer = CountVectorizer( binary=True)         \n",
    "\n",
    "doc_word = vectorizer.fit_transform(df[\"clean_description\"]) # dont need .data cuz we have dataframe\n",
    "# Transform descriptions into a sparse matrix\n",
    "doc_word = ss.csr_matrix(doc_word)\n",
    "\n",
    "doc_word.shape # n_docs x m_words\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#anchored_topic_model = ct.Corex(n_hidden=4, seed=2)\n",
    "#anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/code/nvpsani/topic-modelling-using-guided-lda\n",
    "# used workaround as well\n",
    "\n",
    "import numpy as np\n",
    "from lda import guidedlda as glda\n",
    "\n",
    "model = glda.GuidedLDA(n_topics=4, n_iter=2000, random_state=7, refresh=20,alpha=0.01,eta=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
