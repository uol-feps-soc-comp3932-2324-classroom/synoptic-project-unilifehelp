{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing metadata zip file and converting it to dataframe (based on original code provided by the datasource )\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "def getDF(path):\n",
    "  # Unzip the file, load in each line as an object\n",
    "  g = gzip.open(path, 'rb')\n",
    "  g = [json.loads(l) for l in g]\n",
    "\n",
    "  # Map to a dictionary, then load in as a dataframe\n",
    "  dict_df = {i: d for (i, d) in enumerate(g)}\n",
    "  return pd.DataFrame.from_dict(dict_df, orient='index')\n",
    "\n",
    "df = getDF('meta_ALL_Beauty.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning steps\n",
    "\n",
    "# Selecting only the columns that are required for analysis\n",
    "description_and_asin = [\"description\",\"asin\"]\n",
    "df = df[description_and_asin]\n",
    "\n",
    "# Converting descriptions to a single sting because descriptions are in a list of strings, and removing \\n\n",
    "df[\"description_str\"] = df.description.map(lambda x: \".\".join(x).replace(\"\\n\",\"\"))\n",
    "\n",
    "# Remove empty or null descriptions\n",
    "df = df.drop(df[ ( (df[\"description_str\"] == \"\" ) | ( df[\"description_str\"].isnull()) ) ].index)\n",
    "#15108\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_str</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13743</td>\n",
       "      <td>13743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13743</td>\n",
       "      <td>13743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loud 'N Clear Personal Sound Amplifier allows ...</td>\n",
       "      <td>6546546450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          description_str        asin\n",
       "count                                               13743       13743\n",
       "unique                                              13743       13743\n",
       "top     Loud 'N Clear Personal Sound Amplifier allows ...  6546546450\n",
       "freq                                                    1           1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate asin and descriptions so can have only unique products for classifier\n",
    "df[df.duplicated(\"asin\")]\n",
    "df = df.drop_duplicates(\"asin\", keep=\"last\")\n",
    "\n",
    "df[df.duplicated(\"description_str\")]\n",
    "df = df.drop_duplicates(\"description_str\", keep=\"last\")\n",
    "\n",
    "# Removing redundant old description column\n",
    "description_str_and_asin = [\"description_str\",\"asin\"]\n",
    "df = df[description_str_and_asin]\n",
    "df.describe() #13743\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Identify range of descriptions in after duplicates removal\\ndf_descriptions_without_empty = df[\"description_str\"].apply(lambda x: len(x.split()))\\ndf_descriptions_without_empty.describe([0.1,0.15,0.20,0.25,0.30,0.75,0.85,0.90,0.92,0.95,0.97,0.98,0.99])\\n# 97% upper limit of 259 (same % milit as reviews)\\n# 11 words (same numbe of words as reviews) 20% because no assuption is made on how much more each one is informative as well as descriptions \\n# being a smaller sample\\n#13743'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used later for write up\n",
    "'''# Identify range of descriptions in after duplicates removal\n",
    "df_descriptions_without_empty = df[\"description_str\"].apply(lambda x: len(x.split()))\n",
    "df_descriptions_without_empty.describe([0.1,0.15,0.20,0.25,0.30,0.75,0.85,0.90,0.92,0.95,0.97,0.98,0.99])\n",
    "# 97% upper limit of 259 (same % milit as reviews)\n",
    "# 11 words (same numbe of words as reviews) 20% because no assuption is made on how much more each one is informative as well as descriptions \n",
    "# being a smaller sample\n",
    "#13743'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                  9940\n",
       "unique                                                 9940\n",
       "top       Loud 'N Clear Personal Sound Amplifier allows ...\n",
       "freq                                                      1\n",
       "Name: description_str, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove descriptions within set limit of words\n",
    "\n",
    "# Split descriptions at white spaces\n",
    "df[\"num_words_description\"] = df[\"description_str\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Remove descriptions that don't meet this criteria\n",
    "df = df[(df[\"num_words_description\"] <= 259) & (df[\"num_words_description\"] >= 14)]\n",
    "\n",
    "df[\"description_str\"].describe() #9940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_descriptions_without_empty = df[\"description_str\"].apply(lambda x: len(x.split()))\\ndf_descriptions_without_empty.describe([0.1,0.15,0.20,0.25,0.30,0.75,0.85,0.90,0.92,0.95,0.97,0.98,0.99])'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Later used for write up\n",
    "'''df_descriptions_without_empty = df[\"description_str\"].apply(lambda x: len(x.split()))\n",
    "df_descriptions_without_empty.describe([0.1,0.15,0.20,0.25,0.30,0.75,0.85,0.90,0.92,0.95,0.97,0.98,0.99])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To csv for sense checking - TODO later will be removed\n",
    "df.to_csv(\"Descriptions before cleaning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_str</th>\n",
       "      <th>asin</th>\n",
       "      <th>num_words_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Loud 'N Clear Personal Sound Amplifier allows ...</td>\n",
       "      <td>6546546450</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No7 Stay Perfect Foundation now stays perfect ...</td>\n",
       "      <td>7250468162</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lacto Calamine Skin Balance Daily Nourishing L...</td>\n",
       "      <td>7414204790</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>According to the legend, in 1613, two brothers...</td>\n",
       "      <td>8279996397</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Novi prevod proslavljene knjige Zadruga objavl...</td>\n",
       "      <td>8637910351</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32879</th>\n",
       "      <td>Theres no finer way for a chap to get ready fo...</td>\n",
       "      <td>B01HIH2QTU</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32880</th>\n",
       "      <td>Move over soap on a rope! This heavy-duty Bric...</td>\n",
       "      <td>B01HIHLFOC</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32884</th>\n",
       "      <td>Eau de parfum spray vial mini design house: yv...</td>\n",
       "      <td>B01HIPOQ2M</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32885</th>\n",
       "      <td>Pokemon Plush 9.2 Inch / 23cm Gengar Doll Stuf...</td>\n",
       "      <td>B01HIUEEHO</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32890</th>\n",
       "      <td>Brand new and high quality&lt;br&gt; Enables fast vo...</td>\n",
       "      <td>B01HJASD20</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9940 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description_str        asin  \\\n",
       "0      Loud 'N Clear Personal Sound Amplifier allows ...  6546546450   \n",
       "2      No7 Stay Perfect Foundation now stays perfect ...  7250468162   \n",
       "4      Lacto Calamine Skin Balance Daily Nourishing L...  7414204790   \n",
       "7      According to the legend, in 1613, two brothers...  8279996397   \n",
       "8      Novi prevod proslavljene knjige Zadruga objavl...  8637910351   \n",
       "...                                                  ...         ...   \n",
       "32879  Theres no finer way for a chap to get ready fo...  B01HIH2QTU   \n",
       "32880  Move over soap on a rope! This heavy-duty Bric...  B01HIHLFOC   \n",
       "32884  Eau de parfum spray vial mini design house: yv...  B01HIPOQ2M   \n",
       "32885  Pokemon Plush 9.2 Inch / 23cm Gengar Doll Stuf...  B01HIUEEHO   \n",
       "32890  Brand new and high quality<br> Enables fast vo...  B01HJASD20   \n",
       "\n",
       "       num_words_description  \n",
       "0                         37  \n",
       "2                         96  \n",
       "4                         14  \n",
       "7                         77  \n",
       "8                         53  \n",
       "...                      ...  \n",
       "32879                     21  \n",
       "32880                     18  \n",
       "32884                     21  \n",
       "32885                     15  \n",
       "32890                     48  \n",
       "\n",
       "[9940 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sense check - TODO later will be removed\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'forty', '‘s', 'off', 'around', 'further', 'same', 'throughout', 'amongst', 'does', 'hereupon', 'front', 'being', 'became', 'someone', 'whereas', 'again', 'although', 'will', 'seeming', 'anyhow', 'own', 'amount', 'become', 'however', 'would', 'nor', 'bottom', 'sometimes', 'often', \"'m\", 'over', 'rather', 'very', 'please', '‘m', 'any', 'because', 'if', 'twelve', '‘re', 'via', '’ve', 'within', 'even', 'really', 'full', 'becoming', 'first', 'along', 'regarding', 'fifteen', 'another', 'show', 'nobody', 'most', 'whom', 'on', 'top', 'there', 'out', 'whither', 'hereby', 'an', 'serious', 'in', 'never', 'enough', 'himself', 'whereupon', 'something', 'here', '‘ve', 'neither', 'are', 'between', 'less', 'seemed', 'onto', 'every', 'across', 'three', 'call', 'everywhere', 'also', 'after', 'anyway', \"'s\", 'perhaps', 'per', 'anyone', 'of', 'not', 'than', 'thus', 'their', 'too', 'how', 'these', '‘d', 'beyond', 'several', 'is', 'had', 'afterwards', 'just', 'unless', 'latterly', 'n‘t', 'alone', 'hence', 'fifty', 'this', 'been', 'behind', 'among', 'therein', 'which', 'be', 'thence', 'thereafter', 'everyone', 'somewhere', 'to', 'his', 'it', 'third', 'least', 'move', 'so', 'otherwise', 'few', 'none', 'go', 'others', 'where', 'besides', 'back', 'why', 'during', 'mine', 'five', 'ca', 'part', 'latter', 'done', 'cannot', 'seems', 'six', 'myself', 'a', 'no', 'get', 'its', 'make', 'say', 'did', 'must', 'thru', 'once', 'then', 'still', 'may', 'we', '’s', 'your', 'they', 'yourself', 'whereafter', 'doing', 'beforehand', 'against', 'nine', 'already', 'us', '’ll', 'that', 'namely', 'but', 'keep', 'whereby', 'well', 'such', 'former', 'i', 'and', 'together', \"'ll\", 'about', 'am', 'was', 'or', 'through', 'two', 'name', 'who', 'almost', 'thereby', 'under', 'whatever', 'anything', 'four', 'her', 'ours', 'last', 'sixty', 'while', 'below', 'twenty', 'ten', \"'d\", 'side', 'whoever', \"n't\", 'wherever', 'somehow', 'both', 'eight', 'becomes', 'without', 'empty', 'nevertheless', 'from', 'nowhere', 'with', 'into', 'when', 'seem', 'should', 'could', 'moreover', 'them', 'other', 'has', 'whose', 'my', 'you', 'can', 'whole', 'might', 'toward', 'n’t', 'herself', 'yours', 'themselves', 're', 'everything', 'elsewhere', 'take', 'towards', 'hereafter', 'have', 'whence', 'since', 'yet', 'noone', 'hundred', 'until', 'him', 'our', 'whether', 'up', \"'ve\", 'she', 'wherein', 'above', 'me', 'sometime', 'beside', 'only', 'much', 'now', 'formerly', 'various', 'else', 'put', 'whenever', 'itself', 'were', 'yourselves', 'anywhere', 'either', 'hers', 'he', 'what', 'each', 'though', 'one', 'some', '’m', 'many', 'before', 'ourselves', 'nothing', 'upon', 'as', 'used', 'do', 'always', 'at', 'meanwhile', 'give', 'due', 'except', 'those', 'see', 'eleven', 'for', 'made', '’re', 'all', 'therefore', 'quite', 'the', \"'re\", 'using', 'indeed', 'by', 'thereupon', 'mostly', 'herein', '’d', 'next', 'ever', 'more', '‘ll', 'down'}\n",
      "326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regrex for removing characters\n",
    "import re\n",
    "\n",
    "# ----------------------------- Check which bits i needs still TODO\n",
    "# Spacy for spell check\n",
    "import spacy\n",
    "import contextualSpellCheck\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "contextualSpellCheck.add_to_pipe(nlp)\n",
    "\n",
    "# Spacy stop word creation\n",
    "stopping_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stopping_words_new = stopping_words\n",
    "# -----------------------------\n",
    "print(stopping_words)\n",
    "print(len(stopping_words))\n",
    "\n",
    "# NLTK for tokenization and lemmatization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'forty', '‘s', 'off', 'around', 'further', 'same', 'throughout', 'amongst', 'does', 'hereupon', 'front', 'being', 'became', 'someone', 'whereas', 'again', 'although', 'will', 'seeming', 'anyhow', 'own', 'amount', 'become', 'however', 'would', 'nor', 'bottom', 'sometimes', 'often', \"'m\", 'over', 'rather', 'very', 'please', '‘m', 'any', 'because', 'if', 'twelve', '‘re', 'via', '’ve', 'within', 'even', 'really', 'full', 'becoming', 'first', 'along', 'regarding', 'fifteen', 'another', 'show', 'nobody', 'most', 'whom', 'on', 'top', 'there', 'out', 'whither', 'hereby', 'an', 'serious', 'in', 'never', 'enough', 'himself', 'whereupon', 'something', 'here', '‘ve', 'neither', 'are', 'between', 'less', 'seemed', 'onto', 'every', 'across', 'three', 'call', 'everywhere', 'also', 'after', 'anyway', \"'s\", 'perhaps', 'per', 'anyone', 'of', 'not', 'than', 'thus', 'their', 'too', 'how', 'these', '‘d', 'beyond', 'several', 'is', 'had', 'afterwards', 'just', 'unless', 'latterly', 'n‘t', 'alone', 'hence', 'fifty', 'this', 'been', 'behind', 'among', 'therein', 'which', 'be', 'thence', 'thereafter', 'everyone', 'somewhere', 'to', 'his', 'it', 'third', 'least', 'move', 'so', 'otherwise', 'few', 'none', 'go', 'others', 'where', 'besides', 'back', 'why', 'during', 'mine', 'five', 'ca', 'part', 'latter', 'done', 'cannot', 'seems', 'six', 'myself', 'a', 'get', 'its', 'make', 'say', 'did', 'must', 'thru', 'once', 'then', 'still', 'may', 'we', '’s', 'your', 'they', 'yourself', 'whereafter', 'doing', 'beforehand', 'against', 'nine', 'already', 'us', '’ll', 'that', 'namely', 'but', 'keep', 'whereby', 'well', 'such', 'former', 'i', 'and', 'together', \"'ll\", 'about', 'am', 'was', 'or', 'through', 'two', 'name', 'who', 'almost', 'thereby', 'under', 'whatever', 'anything', 'four', 'her', 'ours', 'last', 'sixty', 'while', 'below', 'twenty', 'ten', \"'d\", 'side', 'whoever', \"n't\", 'wherever', 'somehow', 'both', 'eight', 'becomes', 'without', 'empty', 'nevertheless', 'from', 'nowhere', 'with', 'into', 'when', 'seem', 'should', 'could', 'moreover', 'them', 'other', 'has', 'whose', 'my', 'you', 'can', 'whole', 'might', 'toward', 'n’t', 'herself', 'yours', 'themselves', 're', 'everything', 'elsewhere', 'take', 'towards', 'hereafter', 'have', 'whence', 'since', 'yet', 'noone', 'hundred', 'until', 'him', 'our', 'whether', 'up', \"'ve\", 'she', 'wherein', 'above', 'me', 'sometime', 'beside', 'only', 'much', 'now', 'formerly', 'various', 'else', 'put', 'whenever', 'itself', 'were', 'yourselves', 'anywhere', 'either', 'hers', 'he', 'what', 'each', 'though', 'one', 'some', '’m', 'many', 'before', 'ourselves', 'nothing', 'upon', 'as', 'used', 'do', 'always', 'at', 'meanwhile', 'give', 'due', 'except', 'those', 'see', 'eleven', 'for', 'made', '’re', 'all', 'therefore', 'quite', 'the', \"'re\", 'using', 'indeed', 'by', 'thereupon', 'mostly', 'herein', '’d', 'next', 'ever', 'more', '‘ll', 'down'}\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "# Removing certain stop words from the list of stop words because they are part of our seed terms\n",
    "stopping_words_to_keep = ['no']\n",
    "for word in list(stopping_words_new):\n",
    "    if word in stopping_words_to_keep:\n",
    "        stopping_words_new.remove(word)\n",
    "print(stopping_words_new)\n",
    "print(len(stopping_words_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of keywords\n",
    "\n",
    "def preprocessing_keywords(raw_string):\n",
    "    \n",
    "    # Remove html tags and anything inside them \n",
    "    no_html = re.sub(r'<[^>]*>','', raw_string)\n",
    "\n",
    "    # Make everything lowercase\n",
    "    lowercase_column = no_html.lower()\n",
    "\n",
    "    # TODO have a look at apostrophe again\n",
    "    # Remove apostrophe \n",
    "    without_apostrophe = re.sub(r'[\\']', '', lowercase_column)\n",
    "    \n",
    "    # Remove all non alphabetic instances that aren't a space and replace them with a space \n",
    "    alphabetic_keywords = re.sub(r'[^a-z\\s]', ' ', without_apostrophe)\n",
    "\n",
    "    # Tokenize string into individual words\n",
    "    tokens = word_tokenize(alphabetic_keywords) \n",
    "    \n",
    "    # Remove stopping words\n",
    "    tokens_without_stopping_words = [token for token in tokens if token not in stopping_words_new]\n",
    "\n",
    "    # Lemmatize tokens using nltk and join them into phrases\n",
    "    sentances_without_stop_words = ' '.join([lemmatizer.lemmatize(t) for t in tokens_without_stopping_words])\n",
    "\n",
    "    return sentances_without_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compostable', 'low impact', 'conservation', 'triclosan free', 'no packaging', 'fsc', 'plastic free', 'refillable', 'sustainable', 'cosmos', 'organic', 'recyclable', 'reusable', 'refill', 'renewable', 'recycled', 'environmentally friendly', 'eco', 'no oxybenzone', 'reef safe', 'soil association', 'biodegradable', 'natrue', 'ecological']\n",
      "['non profit', 'leaping bunny', 'cruelty free', 'equality', 'no animal', 'donated', 'peta', 'ethical', 'fair trade']\n",
      "['local farmer', 'community', 'economic prosperity', 'small business', 'fair wage', 'locally sourced', 'renewable', 'recycled', 'local ingredient', 'fair trade']\n",
      "['paraben free', 'non toxic', 'formaldehyde free', 'organic', 'fragrance free', 'no toxic', 'sulfate free', 'phthalates free', 'non nano', 'triclosan free', 'soil association', 'no fragrance', 'usda', 'natrue', 'non gmo', 'cosmos']\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "#                      Cleaning Keywords                                \n",
    "\n",
    "#ngram (1,2)\n",
    "enviromental = ['recyclable', 'recycled', ' Environmentally friendly ','biodegradable', ' no packaging', 'sustainable', ' ecological ', 'plastic-free', 'compostable', 'renewable', 'reusable', 'biodegradable', 'organic', 'refillable', 'refills', 'recycled', 'reef safe',  'no oxybenzone ',  'triclosan-free', 'low-impact', 'soil association', 'conservation', 'COSMOS ', 'NATRUE ', 'FSC', 'eco']   \n",
    "\n",
    "\n",
    "social = ['cruelty-free', 'equality', 'PETA', 'leaping bunny', 'fair trade', 'no animal', 'ethical ', ' Non-profit ', ' Donated ']  \n",
    "\n",
    "\n",
    "economic = ['Fair trade', 'renewable', 'locally sourced', 'small business', 'recycled', ' Fair wage', 'community', ' Economic prosperity ', ' local ingredients ', 'local farmers ']   \n",
    "\n",
    "\n",
    "health = ['non-toxic', 'no toxic', 'organic', 'paraben-free', 'triclosan-free', 'phthalates-free', 'non-nano', 'formaldehyde-free', 'non GMO', 'soil association', 'COSMOS ', 'NATRUE', 'USDA', ' Fragrance-free ', ' no fragrance ', ' sulfate free ']   \n",
    "\n",
    "\n",
    "\n",
    "# Cleaning keywords/phrases \n",
    "enviro_p = list(set(preprocessing_keywords(phrase) for phrase in enviromental))\n",
    "social_p = list(set(preprocessing_keywords(phrase) for phrase in social))\n",
    "economic_p = list(set(preprocessing_keywords(phrase) for phrase in economic))\n",
    "health_p = list(set(preprocessing_keywords(phrase) for phrase in health))\n",
    "\n",
    "print(enviro_p)\n",
    "print(social_p)\n",
    "print(economic_p)\n",
    "print(health_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spell check (creating dictionary)\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# term_index is the column of the term and count_index is the column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# Combining all keywords as individual words\n",
    "anchor_words_combined = list(set([word for sublist in [enviro_p, social_p, economic_p, health_p] for phrase in sublist for word in phrase.split()]))\n",
    "\n",
    "# Add all words to the dictionary\n",
    "for word in anchor_words_combined:\n",
    "    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    if len(suggestions) > 0 and suggestions[0].term == word:\n",
    "        # the best suggestion for this word is itself - it must exist in the dictionary\n",
    "        continue\n",
    "\n",
    "    # add to the dictionary with frequency 1 \n",
    "    sym_spell.create_dictionary_entry(word, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of descriptions\n",
    "\n",
    "def preprocessing(raw_string):\n",
    "    # Remove html tags and anything inside them \n",
    "    no_html = re.sub(r'<[^>]*>','', raw_string)\n",
    "\n",
    "    # Make everything lowercase\n",
    "    lowercase_column = no_html.lower()\n",
    "\n",
    "    # Remove apostrophe for uniformity and to enable spell check to correct words with apostrophe\n",
    "    without_apostrophe = re.sub(r'[\\']', '', lowercase_column)\n",
    "\n",
    "    # Remove all non alphabetic instances that aren't a space and replace them with a space \n",
    "    alphabetic_column = re.sub(r'[^a-z\\s]', ' ', without_apostrophe)\n",
    "\n",
    "    # Tokenize string into individual words\n",
    "    tokens = word_tokenize(alphabetic_column) \n",
    "    \n",
    "    # For each word we will have a list of suggestions\n",
    "    spelling_suggestions = [sym_spell.lookup(x, Verbosity.CLOSEST, max_edit_distance=2) for x in tokens]\n",
    "    # Drop those that have no suggestions and top[0] suggestions for those who do\n",
    "    spelling_suggestions = [x[0].term for x in spelling_suggestions if len(x) > 0]\n",
    "    \n",
    "    # Remove stopping words \n",
    "    tokens_without_stopping_words = [token for token in spelling_suggestions if token not in stopping_words_new]\n",
    "\n",
    "    # Lemmatize tokens (nltk)\n",
    "    sentances_without_stop_words = ' '.join([lemmatizer.lemmatize(t) for t in tokens_without_stopping_words])\n",
    "\n",
    "    return sentances_without_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        loud clear personal sound amplifier allows tur...\n",
       "2        no stay perfect foundation stay perfect longer...\n",
       "4        calamine skin balance daily nourishing lotion ...\n",
       "7        according legend brother native origin black b...\n",
       "8        nov period knife priv colour martini kay divot...\n",
       "                               ...                        \n",
       "32879    no finer way chap ready close gentleman hardwa...\n",
       "32880    soap rope heavy duty brick soap toughest gent ...\n",
       "32884    eau de perfume spray vial mini design house yv...\n",
       "32885    pokemon plush inch pm gear doll stuffed animal...\n",
       "32890    brand new high quality enables fast volley ens...\n",
       "Name: clean_description, Length: 9940, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean descriptions\n",
    "df[\"clean_description\"] = df[\"description_str\"].apply(preprocessing)\n",
    "# Export to CSV for sense checking\n",
    "df[\"clean_description\"].to_csv(\"Clean Descriptions.csv\")\n",
    "\n",
    "df[\"clean_description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any empty descriptions or nulls \n",
    "df = df.drop(df[ ( (df[\"clean_description\"] == \"\" ) | ( df[\"clean_description\"].isnull()) ) ].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_description_without_empty_clean = df[\"clean_description\"].apply(lambda x: len(x.split()))\\ndf_description_without_empty_clean.describe([0.03,0.1,0.2,0.3,0.75,0.85,0.90,0.95]) '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For write up - identifying range of descriptions in after cleaning\n",
    "'''df_description_without_empty_clean = df[\"clean_description\"].apply(lambda x: len(x.split()))\n",
    "df_description_without_empty_clean.describe([0.03,0.1,0.2,0.3,0.75,0.85,0.90,0.95]) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For write up - remove later\n",
    "from collections import Counter\n",
    "#Counter(\" \".join(df[\"clean_description\"]).split()).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>description_str</th>\n",
       "      <th>asin</th>\n",
       "      <th>num_words_description</th>\n",
       "      <th>clean_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Loud 'N Clear Personal Sound Amplifier allows ...</td>\n",
       "      <td>6546546450</td>\n",
       "      <td>37</td>\n",
       "      <td>loud clear personal sound amplifier allows tur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>No7 Stay Perfect Foundation now stays perfect ...</td>\n",
       "      <td>7250468162</td>\n",
       "      <td>96</td>\n",
       "      <td>no stay perfect foundation stay perfect longer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Lacto Calamine Skin Balance Daily Nourishing L...</td>\n",
       "      <td>7414204790</td>\n",
       "      <td>14</td>\n",
       "      <td>calamine skin balance daily nourishing lotion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>According to the legend, in 1613, two brothers...</td>\n",
       "      <td>8279996397</td>\n",
       "      <td>77</td>\n",
       "      <td>according legend brother native origin black b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Novi prevod proslavljene knjige Zadruga objavl...</td>\n",
       "      <td>8637910351</td>\n",
       "      <td>53</td>\n",
       "      <td>nov period knife priv colour martini kay divot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9935</th>\n",
       "      <td>32879</td>\n",
       "      <td>Theres no finer way for a chap to get ready fo...</td>\n",
       "      <td>B01HIH2QTU</td>\n",
       "      <td>21</td>\n",
       "      <td>no finer way chap ready close gentleman hardwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9936</th>\n",
       "      <td>32880</td>\n",
       "      <td>Move over soap on a rope! This heavy-duty Bric...</td>\n",
       "      <td>B01HIHLFOC</td>\n",
       "      <td>18</td>\n",
       "      <td>soap rope heavy duty brick soap toughest gent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9937</th>\n",
       "      <td>32884</td>\n",
       "      <td>Eau de parfum spray vial mini design house: yv...</td>\n",
       "      <td>B01HIPOQ2M</td>\n",
       "      <td>21</td>\n",
       "      <td>eau de perfume spray vial mini design house yv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9938</th>\n",
       "      <td>32885</td>\n",
       "      <td>Pokemon Plush 9.2 Inch / 23cm Gengar Doll Stuf...</td>\n",
       "      <td>B01HIUEEHO</td>\n",
       "      <td>15</td>\n",
       "      <td>pokemon plush inch pm gear doll stuffed animal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9939</th>\n",
       "      <td>32890</td>\n",
       "      <td>Brand new and high quality&lt;br&gt; Enables fast vo...</td>\n",
       "      <td>B01HJASD20</td>\n",
       "      <td>48</td>\n",
       "      <td>brand new high quality enables fast volley ens...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9940 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                    description_str        asin  \\\n",
       "0         0  Loud 'N Clear Personal Sound Amplifier allows ...  6546546450   \n",
       "1         2  No7 Stay Perfect Foundation now stays perfect ...  7250468162   \n",
       "2         4  Lacto Calamine Skin Balance Daily Nourishing L...  7414204790   \n",
       "3         7  According to the legend, in 1613, two brothers...  8279996397   \n",
       "4         8  Novi prevod proslavljene knjige Zadruga objavl...  8637910351   \n",
       "...     ...                                                ...         ...   \n",
       "9935  32879  Theres no finer way for a chap to get ready fo...  B01HIH2QTU   \n",
       "9936  32880  Move over soap on a rope! This heavy-duty Bric...  B01HIHLFOC   \n",
       "9937  32884  Eau de parfum spray vial mini design house: yv...  B01HIPOQ2M   \n",
       "9938  32885  Pokemon Plush 9.2 Inch / 23cm Gengar Doll Stuf...  B01HIUEEHO   \n",
       "9939  32890  Brand new and high quality<br> Enables fast vo...  B01HJASD20   \n",
       "\n",
       "      num_words_description                                  clean_description  \n",
       "0                        37  loud clear personal sound amplifier allows tur...  \n",
       "1                        96  no stay perfect foundation stay perfect longer...  \n",
       "2                        14  calamine skin balance daily nourishing lotion ...  \n",
       "3                        77  according legend brother native origin black b...  \n",
       "4                        53  nov period knife priv colour martini kay divot...  \n",
       "...                     ...                                                ...  \n",
       "9935                     21  no finer way chap ready close gentleman hardwa...  \n",
       "9936                     18  soap rope heavy duty brick soap toughest gent ...  \n",
       "9937                     21  eau de perfume spray vial mini design house yv...  \n",
       "9938                     15  pokemon plush inch pm gear doll stuffed animal...  \n",
       "9939                     48  brand new high quality enables fast volley ens...  \n",
       "\n",
       "[9940 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset index of the data frame for easy use\n",
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation\n",
    "df.to_csv('yyy.csv', index=False)#CLEAN DESCRIPTIONS for Evaluation 11-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258992\n"
     ]
    }
   ],
   "source": [
    "#                       COREX VECTORIZATION\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Setting vectorizer to take phrases in range 1 to 2 (binary for corex)\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range= (1,2)) \n",
    "\n",
    "# Fit the vectorizer and transform it on the description corpus\n",
    "doc_word = vectorizer.fit_transform(df[\"clean_description\"])\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print topics in text file\n",
      "Theme 1: ['organic', 'eco', 'biodegradable', 'recyclable', 'reusable', 'recycled', 'sustainable', 'no way', 'way connected', 'renewable']\n",
      "Theme 2: ['cruelty free', 'no animal', 'fair trade', 'cruelty', 'donated', 'ethical', 'non profit', 'animal testing', 'trimmer', 'cordless']\n",
      "Theme 3: ['recycled', 'fair trade', 'renewable', 'community', 'need', 'dimension', 'safe', 'fit people', 'hair', 'want']\n",
      "Theme 4: ['organic', 'paraben free', 'fragrance free', 'sulfate free', 'skin', 'ingredient', 'oil', 'natural', 'extract', 'help']\n"
     ]
    }
   ],
   "source": [
    "# Code based on https://github.com/gregversteeg/corex_topic/blob/mastera/corextopic/example/corex_topic_example.ipynb\n",
    "import corextopic.corextopic as ct\n",
    "import corextopic.vis_topic as vt\n",
    "\n",
    "anchor_words = [enviro_p, social_p, economic_p, health_p]\n",
    "\n",
    "anchored_topic_model = ct.Corex(n_hidden=4, seed = 11)\n",
    "anchored_topic_model.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=11)\n",
    "\n",
    "# Same results of corex model for evaluation\n",
    "vt.vis_rep(anchored_topic_model, column_label=words, prefix='CX Desc')#FINAL 11-4 Descriptions CorEx Model\n",
    "\n",
    "topics_list = anchored_topic_model.get_topics()\n",
    "\n",
    "# Get top 10 phrases for each topic\n",
    "top_words = []\n",
    "for sublist in topics_list:\n",
    "    phrase = [item[0] for item in sublist]\n",
    "    top_words.append(phrase)\n",
    "\n",
    "for index, phrase in enumerate(top_words):\n",
    "    print(f\"Theme {index+1}:\", phrase)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA # GLDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258992\n"
     ]
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>     GLDA VECTORIZATION         <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Setting vectorizer to take phrases in range 1 to 2 (non binary for GLDA)\n",
    "vectorizer_GLDA = CountVectorizer(binary=False, ngram_range= (1,2))\n",
    "\n",
    "# Fit the vectorizer and transform & fit it on the description corpus\n",
    "vectorised_descriptions_corpus = vectorizer_GLDA.fit_transform(df[\"clean_description\"])\n",
    "# TODO COMMENT\n",
    "word2id = vectorizer_GLDA.vocabulary_ #columns dictionary\n",
    "\n",
    "vocab_GLDA = vectorizer_GLDA.get_feature_names_out() \n",
    "print(len(vocab_GLDA))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 9940\n",
      "INFO:lda:vocab_size: 258992\n",
      "INFO:lda:n_words: 833012\n",
      "INFO:lda:n_topics: 4\n",
      "INFO:lda:n_iter: 2000\n",
      "INFO:lda:<0> log likelihood: -10435791\n",
      "INFO:lda:<50> log likelihood: -9501028\n",
      "INFO:lda:<100> log likelihood: -9494119\n",
      "INFO:lda:<150> log likelihood: -9492199\n",
      "INFO:lda:<200> log likelihood: -9491280\n",
      "INFO:lda:<250> log likelihood: -9491900\n",
      "INFO:lda:<300> log likelihood: -9491143\n",
      "INFO:lda:<350> log likelihood: -9490851\n",
      "INFO:lda:<400> log likelihood: -9491041\n",
      "INFO:lda:<450> log likelihood: -9489746\n",
      "INFO:lda:<500> log likelihood: -9493973\n",
      "INFO:lda:<550> log likelihood: -9491381\n",
      "INFO:lda:<600> log likelihood: -9492189\n",
      "INFO:lda:<650> log likelihood: -9492700\n",
      "INFO:lda:<700> log likelihood: -9489726\n",
      "INFO:lda:<750> log likelihood: -9492319\n",
      "INFO:lda:<800> log likelihood: -9494881\n",
      "INFO:lda:<850> log likelihood: -9491351\n",
      "INFO:lda:<900> log likelihood: -9492681\n",
      "INFO:lda:<950> log likelihood: -9493106\n",
      "INFO:lda:<1000> log likelihood: -9493619\n",
      "INFO:lda:<1050> log likelihood: -9491257\n",
      "INFO:lda:<1100> log likelihood: -9493863\n",
      "INFO:lda:<1150> log likelihood: -9491261\n",
      "INFO:lda:<1200> log likelihood: -9490589\n",
      "INFO:lda:<1250> log likelihood: -9491050\n",
      "INFO:lda:<1300> log likelihood: -9491831\n",
      "INFO:lda:<1350> log likelihood: -9490723\n",
      "INFO:lda:<1400> log likelihood: -9490219\n",
      "INFO:lda:<1450> log likelihood: -9490973\n",
      "INFO:lda:<1500> log likelihood: -9489375\n",
      "INFO:lda:<1550> log likelihood: -9488620\n",
      "INFO:lda:<1600> log likelihood: -9491358\n",
      "INFO:lda:<1650> log likelihood: -9490272\n",
      "INFO:lda:<1700> log likelihood: -9490907\n",
      "INFO:lda:<1750> log likelihood: -9489085\n",
      "INFO:lda:<1800> log likelihood: -9490676\n",
      "INFO:lda:<1850> log likelihood: -9487785\n",
      "INFO:lda:<1900> log likelihood: -9488277\n",
      "INFO:lda:<1950> log likelihood: -9488728\n",
      "INFO:lda:<1999> log likelihood: -9490264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hair, use, color, lip, nail, brush, easy, water, soft, long\n",
      "Topic 1: skin, help, natural, free, oil, formula, ingredient, cream, dry, vitamin\n",
      "Topic 2: size, color, quality, high, pm, new, material, product, design, set\n",
      "Topic 3: oil, fragrance, body, product, scent, soap, blend, essential, natural, note\n"
     ]
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>      Guided - LDA          <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# Based on https://www.kaggle.com/code/nvpsani/topic-modelling-using-guided-lda who used workaround GLDA as well\n",
    "from lda import guidedlda as glda\n",
    "import numpy as np\n",
    "# Anchor words list\n",
    "anchor_words = [enviro_p, social_p, economic_p, health_p]\n",
    "# Defining model\n",
    "model = glda.GuidedLDA(n_topics=4, alpha=12.5, eta=0.1, n_iter=2000, random_state=99, refresh=50)\n",
    "\n",
    "# Topics for the model: create a mapping from feature column index to topic ID\n",
    "anchor_topics = {}\n",
    "for topic_id in range(len(anchor_words)):\n",
    "    key_word_list = anchor_words[topic_id]\n",
    "    for word in key_word_list:\n",
    "        col_index = word2id[word]\n",
    "        anchor_topics[col_index] = topic_id\n",
    "\n",
    "# Train model\n",
    "model.fit(vectorised_descriptions_corpus, seed_topics=anchor_topics, seed_confidence=1.1)\n",
    "\n",
    "# Extract the importance of each feature towards each topic, and display the top ones\n",
    "NUM_TOP_FEATURES = 10\n",
    "words_in_topics = model.topic_word_\n",
    "for topic_id in range(len(words_in_topics)):\n",
    "    # Get the feature importance for this one topic\n",
    "    feature_importance = words_in_topics[topic_id]\n",
    "    # Get the indices of the list when sorted\n",
    "    sorted_indices = np.argsort(feature_importance)\n",
    "    # Reverse to get most important first, and take the most important ones\n",
    "    important_features_indices = sorted_indices[::-1][:NUM_TOP_FEATURES]\n",
    "    # Use our vocab list to map to the feature names\n",
    "    top_words = np.array(vocab_GLDA)[important_features_indices]\n",
    "    \n",
    "    print(f'Topic {topic_id}: {\", \".join(top_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36675832 0.12444992 0.20783941 0.30095235]\n",
      " [0.62246763 0.27847643 0.0504222  0.04863373]\n",
      " [0.23821732 0.64294309 0.04099314 0.07784646]\n",
      " ...\n",
      " [0.04250318 0.0403353  0.0605309  0.85663061]\n",
      " [0.05972019 0.19781134 0.58722203 0.15524643]\n",
      " [0.21776969 0.06547084 0.50432374 0.21243573]]\n",
      "[[0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " ...\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Transform the model to be able to set a threshold and get document topic lables\n",
    "doc_topic = model.transform(vectorised_descriptions_corpus)\n",
    "print(doc_topic)\n",
    "\n",
    "# Set threshold\n",
    "threshold = 0.45\n",
    "\n",
    "doc_topic_thresholded = (doc_topic >= threshold).astype(int)\n",
    "print(doc_topic_thresholded)\n",
    "\n",
    "\n",
    "# Output it\n",
    "df_glda_labels = pd.DataFrame(doc_topic_thresholded)\n",
    "df_glda_labels.to_csv('Desc GLDA.txt', sep='\\t', header=False)#FINAL 11-4 Descriptions GLDA Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
